{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "presidio_transformers_recognizer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70wsUJtutQ8q",
        "outputId": "202917f1-507b-4e55-e063-d6ef3df9e087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 777.4 MB 5.7 kB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/microsoft/presidio.git\n",
        "!pip install presidio-analyzer presidio-anonymizer transformers -q\n",
        "!pip install -U spacy -q\n",
        "!python -m spacy download en_core_web_lg -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import Optional, List, Tuple, Set\n",
        "\n",
        "from presidio_analyzer import (\n",
        "    RecognizerResult,\n",
        "    EntityRecognizer,\n",
        "    AnalysisExplanation,\n",
        ")\n",
        "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "class TransformersRecognizer(EntityRecognizer):\n",
        "    \"\"\"\n",
        "    Wrapper for a transformers model, if needed to be used within Presidio Analyzer.\n",
        "    \"\"\"\n",
        "\n",
        "    ENTITIES = [\n",
        "        \"LOCATION\",\n",
        "        \"PERSON\",\n",
        "        \"ORGANIZATION\",\n",
        "        # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.\n",
        "    ]\n",
        "\n",
        "    DEFAULT_EXPLANATION = \"Identified as {} by a Named Entity Recognition using transformers\"\n",
        "\n",
        "    CHECK_LABEL_GROUPS = [\n",
        "        ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),\n",
        "        ({\"PERSON\"}, {\"PER\", \"PERSON\"}),\n",
        "        ({\"ORGANIZATION\"}, {\"ORG\"}),\n",
        "        # ({\"MISCELLANEOUS\"}, {\"MISC\"}), # Probably not PII\n",
        "    ]\n",
        "\n",
        "    MODEL_LANGUAGES = {\n",
        "        \"en\": \"elastic/distilbert-base-uncased-finetuned-conll03-english\",\n",
        "        \"es\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "        \"de\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "        \"ar\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "        \"fr\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "        \"it\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "        \"nl\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "        \"lv\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "    }\n",
        "\n",
        "    PRESIDIO_EQUIVALENCES = {\n",
        "        \"PER\": \"PERSON\",\n",
        "        \"LOC\": \"LOCATION\",\n",
        "        \"ORG\": \"ORGANIZATION\",\n",
        "        # 'MISC': 'MISCELLANEOUS'   # - Probably not PII\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        supported_language: str = \"en\",\n",
        "        supported_entities: Optional[List[str]] = None,\n",
        "        check_label_groups: Optional[Tuple[Set, Set]] = None,\n",
        "        model: str = None,\n",
        "    ):\n",
        "        self.check_label_groups = (\n",
        "            check_label_groups if check_label_groups else self.CHECK_LABEL_GROUPS\n",
        "        )\n",
        "\n",
        "        self.model = pipeline(\"ner\", model=self.MODEL_LANGUAGES.get(supported_language), aggregation_strategy=\"first\")\n",
        "\n",
        "        all_labels = list(self.model.model.config.id2label.values())\n",
        "        self.ENTITIES = list(set([x[2:] for x in all_labels if x!=\"O\" and (x.startswith(\"B-\") or x.startswith(\"I-\"))]))\n",
        "\n",
        "        supported_entities = supported_entities if supported_entities else self.ENTITIES\n",
        "\n",
        "        super().__init__(\n",
        "            supported_entities=supported_entities,\n",
        "            supported_language=supported_language,\n",
        "            name=\"Transformer Analytics\",\n",
        "        )\n",
        "\n",
        "    def load(self) -> None:\n",
        "        \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_supported_entities(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Return supported entities by this model.\n",
        "        :return: List of the supported entities.\n",
        "        \"\"\"\n",
        "        return self.supported_entities\n",
        "\n",
        "    # Class to use Flair with Presidio as an external recognizer.\n",
        "    def analyze(\n",
        "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n",
        "    ) -> List[RecognizerResult]:\n",
        "        \"\"\"\n",
        "        Analyze text using Text Analytics.\n",
        "        :param text: The text for analysis.\n",
        "        :param entities: Not working properly for this recognizer.\n",
        "        :param nlp_artifacts: Not used by this recognizer.\n",
        "        :param language: Text language. Supported languages in MODEL_LANGUAGES\n",
        "        :return: The list of Presidio RecognizerResult constructed from the recognized\n",
        "            Flair detections.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        model_results = self.model(text)\n",
        "\n",
        "        # If there are no specific list of entities, we will look for all of it.\n",
        "        if not entities:\n",
        "            entities = self.supported_entities\n",
        "\n",
        "        recognizer_results = []\n",
        "        for result in model_results:\n",
        "            entity = result[\"entity_group\"]\n",
        "\n",
        "            if entity not in self.supported_entities:\n",
        "                continue\n",
        "\n",
        "            if not self.__check_label(entity, self.check_label_groups):\n",
        "                continue\n",
        "\n",
        "        \n",
        "            textual_explanation = self.DEFAULT_EXPLANATION.format(\n",
        "                entity\n",
        "            )\n",
        "            explanation = self.build_explanation(\n",
        "                round(result[\"score\"], 2), textual_explanation\n",
        "            )\n",
        "            result = self._convert_to_recognizer_result(result, explanation)\n",
        "\n",
        "            recognizer_results.append(result)\n",
        "\n",
        "        return recognizer_results\n",
        "\n",
        "    def _convert_to_recognizer_result(self, entity, explanation) -> RecognizerResult:\n",
        "\n",
        "        entity_type = self.PRESIDIO_EQUIVALENCES.get(entity[\"entity_group\"], entity[\"entity_group\"])\n",
        "        score = round(entity[\"score\"], 2)\n",
        "\n",
        "        flair_results = RecognizerResult(\n",
        "            entity_type=entity_type,\n",
        "            start=entity[\"start\"],\n",
        "            end=entity[\"end\"],\n",
        "            score=score,\n",
        "            analysis_explanation=explanation,\n",
        "        )\n",
        "\n",
        "        return flair_results\n",
        "\n",
        "    def build_explanation(\n",
        "        self, original_score: float, explanation: str\n",
        "    ) -> AnalysisExplanation:\n",
        "        \"\"\"\n",
        "        Create explanation for why this result was detected.\n",
        "        :param original_score: Score given by this recognizer\n",
        "        :param explanation: Explanation string\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        explanation = AnalysisExplanation(\n",
        "            recognizer=self.__class__.__name__,\n",
        "            original_score=original_score,\n",
        "            textual_explanation=explanation,\n",
        "        )\n",
        "        return explanation\n",
        "\n",
        "    @staticmethod\n",
        "    def __check_label(\n",
        "        entity: str,  check_label_groups: Tuple[Set, Set]\n",
        "    ) -> bool:\n",
        "        return any(\n",
        "            [entity in egrp or entity in lgrp for egrp, lgrp in check_label_groups]\n",
        "        )"
      ],
      "metadata": {
        "id": "FzdWfM_JtWJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
        "\n",
        "transformers_recognizer = (\n",
        "    TransformersRecognizer()\n",
        ")  # This would download a very large (+2GB) model on the first run\n",
        "\n",
        "registry = RecognizerRegistry()\n",
        "registry.add_recognizer(transformers_recognizer)\n",
        "\n",
        "analyzer = AnalyzerEngine(registry=registry, )\n",
        "\n",
        "results = analyzer.analyze(\n",
        "    \"My name is Christopher and I live in Irbid.\",\n",
        "    language=\"en\",\n",
        "    return_decision_process=True,\n",
        ")\n",
        "for result in results:\n",
        "    print(result)\n",
        "    print(result.analysis_explanation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHAK3Atm9NQc",
        "outputId": "d58e4d5f-e19d-48a9-8eb3-b8ca5f0617ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type: PERSON, start: 11, end: 22, score: 1.0\n",
            "{'recognizer': 'TransformersRecognizer', 'pattern_name': None, 'pattern': None, 'original_score': 1.0, 'score': 1.0, 'textual_explanation': 'Identified as PER by a Named Entity Recognition using transformers', 'score_context_improvement': 0, 'supportive_context_word': '', 'validation_result': None}\n",
            "type: LOCATION, start: 37, end: 42, score: 0.5\n",
            "{'recognizer': 'TransformersRecognizer', 'pattern_name': None, 'pattern': None, 'original_score': 0.5, 'score': 0.5, 'textual_explanation': 'Identified as LOC by a Named Entity Recognition using transformers', 'score_context_improvement': 0, 'supportive_context_word': '', 'validation_result': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OfDwKJGx-rJz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}